{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Convolutional Neural Networks( or CovNets) are mainly used for image classification. They are also used for NLP. But in this notebook we'll classify handwritten digts (MNIST). Let's dive right in! \n",
    "#### MNIST Dataset Example\n",
    "![MNIST_Dataset](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/220px-MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're gonna build this model with tensorflow. Tensorflow is a library to build models. We don't have that much time to look into math and build models. It makes our work easier. Calculating hidden errors. etc.. The tensoflow API is really beautiful. You'll love it! Let's get into the code \n",
    "![](http://klaatuveratanecto.com/wp-content/uploads/2017/09/markblogtensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Analogy for using tensorflow\n",
    "-->\n",
    "###### If you wanna build something out of lego you don't build the lego blocks, instead what you do is you buy the lego set and build something amazing\n",
    "##### - Future Daniel Bourke\n",
    "Chekout his [channel](https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ) for AI vlogs, Resources, ways to learn and so much more! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install tensorflow # To install tensorflow.This should be done in cmd or a terminal\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more informations on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow # To install tensorflow.This should be done in cmd or a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #Importing the and calling it tf so that we dont need to write a big name everytime~tensorflow\n",
    "#(We're lazy)\n",
    "from tensorflow.examples.tutorials.mnist import input_data #tensorflow readily provides the dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) #storing the dataset in a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set one_hot to True\n",
    "What's one_hot? Checkout this explanation\n",
    "--> https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New to tensorflow? \n",
    "If you're new to tensorflow checkout this \n",
    "--> https://www.datacamp.com/community/tutorials/tensorflow-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these images, our neural net will be able classify and give us the probability of each image being a certain class. Our network should look like this \n",
    "![](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1508999490138.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10 #Number of classes\n",
    "# Let's define our weights\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "x = tf.placeholder(tf.float32, [None, 784]) #None indicates batch_size which is the number of samples you train your model\n",
    "# ^ is for the data(Images)\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])#True output\n",
    "#Creating placeholders will let us define them while creating a tensorflow session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHY CONVNETS?\n",
    "Using regular neural network( Ive written about them in [this](https://github.com/Developer404/Neural_Network) notebook) is fine but you require more computational power( for 1080p images, you require 1166400 weights!!). We dont have that much computational power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "#### ~Convolution\n",
    "Instead of fully connected layers. We us ecovnets, Input is a image.\n",
    "For a computer the image is just an array of numbers with no meaning in it. It's just a matrix of numbers\n",
    "![](http://blog.kleinproject.org/wp-content/uploads/2012/04/sample-matrix.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we take a kernel(aka filters)[Weights] of some size and multiply(and add) it all over the image( Element wise multiplication) Like this. \n",
    "##### Gif that shows convolution with a stride of 1 ![](http://deeplearning.net/software/theano/_images/numerical_no_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some terms to Remember ;)\n",
    "###### Stride: \n",
    "It's the number of times we move the filter[aka Weights] on the image.\n",
    "###### Zero Padding:\n",
    "Sometimes we don't use convolution to make the image smaller. So, we use zero padding to avoid that by padding the borders by zero ie.. Putting zeros around the image and then performing convolution. Here's a gif\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/28094927/padding.gif).\n",
    "And we get the output size same as the input size (without zero-padding).\n",
    "###### Valid padding:\n",
    "Valid padding can be referred to as noremal convolution. You don't but zeros around the borders of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~Relu\n",
    "As we perform activation in regural nets, we do it here too. We perform relu activation and what it does is very simple , it just turns the negative values to 0. And keeps the rest of the postive numbers same\n",
    "![](https://lazyprogrammer.me/wp-content/uploads/2017/10/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\"), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~Maxpooling\n",
    "Maxpooling is a way to reduce dimension=[computational complexity] as it selects the largest element from the window of size that we define. And we also define the number of strides\n",
    "![](https://shafeentejani.github.io/assets/images/pooling.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, ksize=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get into the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 651.818054, Training Accuracy= 0.14062\n",
      "Iter 2560, Minibatch Loss= 747.345459, Training Accuracy= 0.08594\n",
      "Iter 3840, Minibatch Loss= 202.348450, Training Accuracy= 0.06250\n",
      "Iter 5120, Minibatch Loss= 154.696594, Training Accuracy= 0.10938\n",
      "Iter 6400, Minibatch Loss= 138.584885, Training Accuracy= 0.07812\n",
      "Iter 7680, Minibatch Loss= 54.935440, Training Accuracy= 0.10938\n",
      "Iter 8960, Minibatch Loss= 133.113525, Training Accuracy= 0.08594\n",
      "Iter 10240, Minibatch Loss= 243.589310, Training Accuracy= 0.10156\n",
      "Iter 11520, Minibatch Loss= 258.163239, Training Accuracy= 0.10156\n",
      "Iter 12800, Minibatch Loss= 515.287720, Training Accuracy= 0.07031\n",
      "Iter 14080, Minibatch Loss= 210.003235, Training Accuracy= 0.08594\n",
      "Iter 15360, Minibatch Loss= 330.890869, Training Accuracy= 0.10938\n",
      "Iter 16640, Minibatch Loss= 181.708038, Training Accuracy= 0.11719\n",
      "Iter 17920, Minibatch Loss= 136.839355, Training Accuracy= 0.08594\n",
      "Iter 19200, Minibatch Loss= 107.965088, Training Accuracy= 0.08594\n",
      "Iter 20480, Minibatch Loss= 63.337784, Training Accuracy= 0.05469\n",
      "Iter 21760, Minibatch Loss= 158.697845, Training Accuracy= 0.11719\n",
      "Iter 23040, Minibatch Loss= 341.950653, Training Accuracy= 0.09375\n",
      "Iter 24320, Minibatch Loss= 147.734283, Training Accuracy= 0.10156\n",
      "Iter 25600, Minibatch Loss= 199.478546, Training Accuracy= 0.09375\n",
      "Iter 26880, Minibatch Loss= 142.588196, Training Accuracy= 0.14062\n",
      "Iter 28160, Minibatch Loss= 118.694778, Training Accuracy= 0.07812\n",
      "Iter 29440, Minibatch Loss= 158.060272, Training Accuracy= 0.07812\n",
      "Iter 30720, Minibatch Loss= 326.967957, Training Accuracy= 0.07812\n",
      "Iter 32000, Minibatch Loss= 240.018097, Training Accuracy= 0.07031\n",
      "Iter 33280, Minibatch Loss= 102.419037, Training Accuracy= 0.08594\n",
      "Iter 34560, Minibatch Loss= 162.074936, Training Accuracy= 0.10156\n",
      "Iter 35840, Minibatch Loss= 134.409653, Training Accuracy= 0.06250\n",
      "Iter 37120, Minibatch Loss= 169.997406, Training Accuracy= 0.09375\n",
      "Iter 38400, Minibatch Loss= 169.534149, Training Accuracy= 0.08594\n",
      "Iter 39680, Minibatch Loss= 174.338058, Training Accuracy= 0.10156\n",
      "Iter 40960, Minibatch Loss= 153.479187, Training Accuracy= 0.14062\n",
      "Iter 42240, Minibatch Loss= 140.515015, Training Accuracy= 0.07812\n",
      "Iter 43520, Minibatch Loss= 142.858154, Training Accuracy= 0.17969\n",
      "Iter 44800, Minibatch Loss= 200.051743, Training Accuracy= 0.11719\n",
      "Iter 46080, Minibatch Loss= 48.063660, Training Accuracy= 0.07031\n",
      "Iter 47360, Minibatch Loss= 154.801361, Training Accuracy= 0.09375\n",
      "Iter 48640, Minibatch Loss= 169.432312, Training Accuracy= 0.17969\n",
      "Iter 49920, Minibatch Loss= 129.818604, Training Accuracy= 0.14062\n",
      "Iter 51200, Minibatch Loss= 125.623688, Training Accuracy= 0.13281\n",
      "Iter 52480, Minibatch Loss= 118.776993, Training Accuracy= 0.12500\n",
      "Iter 53760, Minibatch Loss= 279.480591, Training Accuracy= 0.04688\n",
      "Iter 55040, Minibatch Loss= 298.253784, Training Accuracy= 0.11719\n",
      "Iter 56320, Minibatch Loss= 207.982971, Training Accuracy= 0.06250\n",
      "Iter 57600, Minibatch Loss= 144.697540, Training Accuracy= 0.11719\n",
      "Iter 58880, Minibatch Loss= 133.746384, Training Accuracy= 0.14062\n",
      "Iter 60160, Minibatch Loss= 77.834625, Training Accuracy= 0.05469\n",
      "Iter 61440, Minibatch Loss= 135.349365, Training Accuracy= 0.10938\n",
      "Iter 62720, Minibatch Loss= 131.063644, Training Accuracy= 0.08594\n",
      "Iter 64000, Minibatch Loss= 144.616486, Training Accuracy= 0.08594\n",
      "Iter 65280, Minibatch Loss= 191.713806, Training Accuracy= 0.07812\n",
      "Iter 66560, Minibatch Loss= 252.014511, Training Accuracy= 0.10938\n",
      "Iter 67840, Minibatch Loss= 197.808411, Training Accuracy= 0.12500\n",
      "Iter 69120, Minibatch Loss= 79.193153, Training Accuracy= 0.11719\n",
      "Iter 70400, Minibatch Loss= 124.645485, Training Accuracy= 0.08594\n",
      "Iter 71680, Minibatch Loss= 156.226334, Training Accuracy= 0.10156\n",
      "Iter 72960, Minibatch Loss= 139.514954, Training Accuracy= 0.11719\n",
      "Iter 74240, Minibatch Loss= 114.138474, Training Accuracy= 0.13281\n",
      "Iter 75520, Minibatch Loss= 176.111008, Training Accuracy= 0.11719\n",
      "Iter 76800, Minibatch Loss= 170.915298, Training Accuracy= 0.07031\n",
      "Iter 78080, Minibatch Loss= 96.675606, Training Accuracy= 0.06250\n",
      "Iter 79360, Minibatch Loss= 193.461975, Training Accuracy= 0.12500\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 128 #The number of samples we want our model to train on \n",
    "learning_rate = 0.5#How fast our model coverges\n",
    "training_iters = 300000#training iterations\n",
    "display_step = 10#How often we want to display the accuracy\n",
    "def model(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    maxpool1 = maxpool2d(conv1)\n",
    "    conv2 = conv2d(maxpool1, weights['wc2'], biases['bc2'])\n",
    "    maxpool2 = maxpool2d(conv2)\n",
    "    maxpool2 = tf.layers.flatten(maxpool2) \n",
    "    fc1 = tf.add(tf.matmul(maxpool2, weights['wd1']),biases['bd1'])\n",
    "    logits = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return logits\n",
    "pred = model(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       })\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              })\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beauty of backpropagation\n",
    "While training, our model learns features of images. When we printed out the feature maps(Output of convolution). We found out that These are the same images but a little blurred. Checkout [this](http://cs231n.stanford.edu/) neural network on the top of the page which gives the feature maps. We refers to researchers mainly.. Yann Lecun. Who is one of inspirations!\n",
    "![](http://www.i-programmer.info/images/stories/News/2016/Nov/A/yannpic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be understandable but still confusing. I'm here to help. One of my new year's resolution was to make youtube videos on my [channel](https://www.youtube.com/channel/UCFViGhziBdf6BDYIkeQeqQw). And I promise I'm gonna try my best to explain. And the explanations will be much more clearer. I hope you liked the Notebook! You could ask me questions on this notebook and also can suggest what changes I have to make and recommend new topics and just become friends by emailing me at shaikasad17@gmail.com\n",
    "And I'll try my best to reply! \n",
    "Youtube videos coming soon..\n",
    "If you want to get started I'd recommend my last [repo](github.com/Developer404/Neural_Network) for understanding simple feed forward neural networks\n",
    "And until then, Cheerio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, I did not train the network fully and go ahead and train it fully by running the cell!. The accuracy is also pretty bad I'm gonna update the notebook by Discussing some techniques of how to prevent overfitting and improve our model by applying dropout, regularization, etc.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
